import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 1. Charger les données
print("Chargement des données...")
df = pd.read_csv("spam.csv", encoding='latin-1')[["v1", "v2"]]
df.columns = ['label', 'text']
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

# 2. Séparer les données
x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# 3. Tokenisation et séquençage
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(x_train)

x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

x_train_pad = pad_sequences(x_train_seq, maxlen=100)
x_test_pad = pad_sequences(x_test_seq, maxlen=100)

# 4. Création du modèle
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 16, input_length=100),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 5. Entraînement
print("Entraînement du modèle...")
model.fit(x_train_pad, y_train, epochs=5, validation_data=(x_test_pad, y_test))

# 6. Évaluation
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"\nPrécision sur les données de test : {acc:.4f}")

# 7. Sauvegarde
model.save("spam_classifier_model.h5")
print("Modèle sauvegardé sous spam_classifier_model.h5")

# 8. Démonstration sur un exemple
def predict_message(msg):
    seq = tokenizer.texts_to_sequences([msg])
    padded = pad_sequences(seq, maxlen=100)
    prediction = model.predict(padded)[0][0]
    return "SPAM" if prediction > 0.5 else "HAM"

# Exemple
message = "Congratulations! You've won a $1000 gift card. Click here to claim."
print("\nExemple de message :")
print(f"Message: \"{message}\"")
print("Prédiction :", predict_message(message))
